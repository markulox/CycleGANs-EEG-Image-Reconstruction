        # DISCRIMINATOR TRAINING SECTION
        # print("UPDATING DISCRIM")
        d1_op.zero_grad()
        d2_op.zero_grad()

        # Reshape the tensor corresponding to the generator and detach everything
        fy_p = fy_p.squeeze(1).detach()
        ly_p = ly_p.squeeze(1).detach()
        fx_u = fx_u.squeeze(1).detach()
        lx_u = lx_u.squeeze(1).detach()

        # Since some of batch might get reduce due to end of iteration
        curr_BS = y_p.shape[0]

        x_p_gen = G.forward(z=torch.rand(curr_BS, Generator.EXPECTED_NOISE).to(DEV), semantic=fy_p, label=ly_p)
        x_p_gen_dtch = x_p_gen.detach()
        x_u_gen = G.forward(z=torch.rand(curr_BS, Generator.EXPECTED_NOISE).to(DEV), semantic=fx_u, label=lx_u)
        x_u_gen_dtch = x_u_gen.detach()

        l1 = l1_loss(d1, x_p, x_p_gen_dtch)
        l2 = l2_loss(d2, x_p, x_p_gen_dtch)
        l3 = l3_loss(d1, x_u, x_u_gen_dtch)
        l4 = l4_loss(d2, x_u, x_u_gen_dtch)

        dl_loss = -((ld1 * l1) + l2 + (ld2 * l3) + l4)
        check_nan(dl_loss.item(), dl1=l1.item(), dl2=l2.item(), dl3=l3.item(), dl4=l4.item())
        dl_loss.backward()

        d1_op.step()
        d2_op.step()

        # GENERATOR TRAINING SECTION
        # print("UPDATING GENERATOR")
        G_op.zero_grad()

        l1 = l1_loss(d1, x_p, x_p_gen, train_gen=True)
        l2 = l2_loss(d2, x_p, x_p_gen, train_gen=True)
        l3 = l3_loss(d1, x_u, x_u_gen, train_gen=True)
        l4 = l4_loss(d2, x_u, x_u_gen, train_gen=True)

        gl_loss = -((ld1 * l1) + l2 + (ld2 * l3) + l4)
        check_nan(gl_loss.item(), gl1=l1.item(), gl2=l2.item(), gl3=l3.item(), gl4=l4.item())
        gl_loss.backward()

        G_op.step()

        # Logging the progress
        batches_done = epch * len(dat_loader) + i
        batches_left = EPCH * len(dat_loader) - batches_done
        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))
        prev_time = time.time()

        sys.stdout.write(
            "\r[Epoch %d/%d] [Batch %d/%d] [J loss: %f] [D loss: %f] [G loss: %f] ETA: %s"
            % (
                epch,
                EPCH,
                i,
                len(dat_loader),
                j_loss.item(),
                dl_loss.item(),
                gl_loss.item(),
                time_left,
            )
        )
        # print(j_loss.item(), dl_loss.item())

        if SAMPLE_INTERVAL != -1 and epch % SAMPLE_INTERVAL == 0 and i == 0:
            sample_images(epch)

        if CHCK_PNT_INTERVAL != -1 and epch % CHCK_PNT_INTERVAL == 0 and i == 0:
            # Save model checkpoints
            torch.save(G.state_dict(), "saved_models/%s/G_%d.pth" % (dataset.get_name(), epch))
            torch.save(d1.state_dict(), "saved_models/%s/d1_%d.pth" % (dataset.get_name(), epch))
            torch.save(d2.state_dict(), "saved_models/%s/d2_%d.pth" % (dataset.get_name(), epch))
            torch.save(sx.state_dict(), "saved_models/%s/sx_%d.pth" % (dataset.get_name(), epch))
            torch.save(sy.state_dict(), "saved_models/%s/sy_%d.pth" % (dataset.get_name(), epch))